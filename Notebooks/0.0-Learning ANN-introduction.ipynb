{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Tensorflow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# py -m pip install -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.10.0', '2.10.0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__, keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work with fashion MINIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 3s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "fashion_mnits = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnits.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a validation dataset, and scale the pixel intesities diving them by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fashion MNIST we need the list of class names to know what we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to create the model using the sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential() # Sequential implies that the layers have to be added one by one\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28])) # n inputs equal to number of features\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\")) # First layer, with 300 neurons\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\")) # Second layer, with 100 neurons\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\")) # Softmax because is a multiclass, and 10 neurons one per class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We build the first layer and add it to the model. It is a Flatten layer whose\n",
    "role is to convert each input image into a 1D array: if it receives input data X, it\n",
    "computes X.reshape(-1, 1). This layer does not have any parameters; it is just\n",
    "there to do some simple preprocessing. Since it is the first layer in the model, you\n",
    "should specify the input_shape, which doesnâ€™t include the batch size, only the\n",
    "shape of the instances. Alternatively, you could add a keras.layers.InputLayer\n",
    "as the first layer, setting input_shape=[28,28].\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Dense layer manages its own weight matrix, containing all the\n",
    "connection weights between the neurons and their inputs. It also manages a vector\n",
    "of bias terms (one per neuron)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other way to create a sequential model is specifying the neurons inside the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "keras.layers.Flatten(input_shape=[28, 28]),\n",
    "keras.layers.Dense(300, activation=\"relu\"),\n",
    "keras.layers.Dense(100, activation=\"relu\"),\n",
    "keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Output shape, the first element is None, that means the size of the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note avery number of params is the last numbers of neurons plus the bias parameters (one per neuron) (Last Neurons * Current neurons) + One per current neurons bias parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the parameters of a layer can be accessed using its get_weights() and\n",
    "set_weights() methods. For a Dense layer, this includes both the connection weights\n",
    "and the bias terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]\n",
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07064635, -0.05387472,  0.05562678, ..., -0.03587768,\n",
       "        -0.05028726, -0.03732417],\n",
       "       [ 0.04496154,  0.03599848,  0.05570811, ..., -0.0265836 ,\n",
       "        -0.0688344 , -0.02863748],\n",
       "       [-0.01820023, -0.02965118,  0.04464071, ..., -0.06666005,\n",
       "         0.03093608, -0.00933824],\n",
       "       ...,\n",
       "       [-0.0209837 ,  0.02819224, -0.04697947, ..., -0.0038941 ,\n",
       "         0.02423908, -0.05529836],\n",
       "       [ 0.03328946, -0.02601187,  0.01839277, ..., -0.0073881 ,\n",
       "         0.02913884,  0.04237702],\n",
       "       [ 0.00095125, -0.0282984 ,  0.03601219, ..., -0.03511011,\n",
       "         0.01155376, -0.01519923]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinh that the model has not been created yet, so, the last values are random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to compile the model in order to specify the loss function and the optimizer to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "optimizer=\"sgd\",\n",
    "metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is ready to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.7166 - accuracy: 0.7626 - val_loss: 0.5279 - val_accuracy: 0.8164\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4877 - accuracy: 0.8296 - val_loss: 0.4501 - val_accuracy: 0.8508\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4427 - accuracy: 0.8441 - val_loss: 0.4154 - val_accuracy: 0.8584\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4145 - accuracy: 0.8555 - val_loss: 0.4142 - val_accuracy: 0.8576\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3937 - accuracy: 0.8624 - val_loss: 0.3865 - val_accuracy: 0.8670\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3785 - accuracy: 0.8665 - val_loss: 0.3774 - val_accuracy: 0.8690\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3645 - accuracy: 0.8709 - val_loss: 0.3624 - val_accuracy: 0.8714\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3541 - accuracy: 0.8744 - val_loss: 0.3514 - val_accuracy: 0.8762\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3425 - accuracy: 0.8786 - val_loss: 0.3464 - val_accuracy: 0.8796\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3335 - accuracy: 0.8805 - val_loss: 0.3376 - val_accuracy: 0.8806\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3243 - accuracy: 0.8835 - val_loss: 0.3488 - val_accuracy: 0.8778\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3163 - accuracy: 0.8859 - val_loss: 0.3288 - val_accuracy: 0.8850\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3102 - accuracy: 0.8888 - val_loss: 0.3281 - val_accuracy: 0.8834\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3022 - accuracy: 0.8913 - val_loss: 0.3211 - val_accuracy: 0.8862\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2962 - accuracy: 0.8931 - val_loss: 0.3112 - val_accuracy: 0.8884\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2897 - accuracy: 0.8948 - val_loss: 0.3235 - val_accuracy: 0.8814\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2838 - accuracy: 0.8980 - val_loss: 0.3185 - val_accuracy: 0.8862\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2783 - accuracy: 0.8998 - val_loss: 0.3068 - val_accuracy: 0.8914\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2737 - accuracy: 0.9004 - val_loss: 0.3082 - val_accuracy: 0.8920\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2676 - accuracy: 0.9037 - val_loss: 0.3183 - val_accuracy: 0.8884\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2634 - accuracy: 0.9051 - val_loss: 0.3047 - val_accuracy: 0.8922\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2582 - accuracy: 0.9064 - val_loss: 0.3089 - val_accuracy: 0.8866\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2545 - accuracy: 0.9079 - val_loss: 0.2975 - val_accuracy: 0.8948\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2491 - accuracy: 0.9110 - val_loss: 0.3172 - val_accuracy: 0.8888\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2452 - accuracy: 0.9118 - val_loss: 0.3034 - val_accuracy: 0.8924\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2415 - accuracy: 0.9124 - val_loss: 0.3076 - val_accuracy: 0.8904\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2375 - accuracy: 0.9147 - val_loss: 0.3024 - val_accuracy: 0.8902\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2327 - accuracy: 0.9159 - val_loss: 0.2916 - val_accuracy: 0.8956\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2291 - accuracy: 0.9178 - val_loss: 0.2919 - val_accuracy: 0.8978\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2263 - accuracy: 0.9187 - val_loss: 0.2921 - val_accuracy: 0.8948\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training set was very skewed, with some classes being overrepresented and others\n",
    "underrepresented, it would be useful to set the class_weight argument when\n",
    "calling the fit() method, which would give a larger weight to underrepresented\n",
    "classes and a lower weight to overrepresented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When plotting the training curve, it should be shifted by half an\n",
    "epoch to the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 64.5304 - accuracy: 0.8414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[64.53044128417969, 0.8414000272750854]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to predict with the model, the proccess is like the other algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 89ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to build a regression model using sequential API again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tha data set will be got of sklearn so, it's good moment to install it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is californa housing (obviously ðŸ˜’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call data\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main diffences with classification task is that for regression we only need a single output neuron (If we want to predict a single value) and uses no activation function. And of course the loss function in mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 3s 6ms/step - loss: 0.8954 - val_loss: 20.1964\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 1.0242 - val_loss: 0.5342\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4416 - val_loss: 0.4373\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3986 - val_loss: 0.4061\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3811 - val_loss: 0.3835\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3710 - val_loss: 0.3758\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3656 - val_loss: 0.3744\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3645 - val_loss: 0.3695\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3614 - val_loss: 0.3673\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3577 - val_loss: 0.3643\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3589 - val_loss: 0.4816\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3550 - val_loss: 0.3613\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3534 - val_loss: 0.3711\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3515 - val_loss: 0.3625\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3488 - val_loss: 0.3836\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3487 - val_loss: 0.3640\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3476 - val_loss: 0.3619\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3446 - val_loss: 0.3581\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3427 - val_loss: 0.3643\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3424 - val_loss: 0.3593\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.3742\n",
      "1/1 [==============================] - 0s 182ms/step\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer='sgd')\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data = (X_valid, y_valid))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3] # pretend these are new instances\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.6883545 ],\n",
       "       [0.64430714],\n",
       "       [1.0672438 ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Models using the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example of a complex model is the Wide & Deep neural network. It connects all or part of the inputs directly to the output layer. \"This architecture makes it possible for the neural network to learn both\n",
    "deep patterns (using the deep path) and simple rules (through the short path).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build such neural network for california hpusing problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_) # Note that we are adding the last layer to this\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1) # and now we only need add the last layer\n",
    "concat = keras.layers.Concatenate()([input_, hidden2]) # Here, in concat it's necessary add the input again, this is the trick\n",
    "output = keras.layers.Dense(1)(concat) # But to the output only add the conat layer, recall output layer doesn't need activation function\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steeps are equal like before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everithing is good but do you remember we said that is possible send all or PART of the input? what if we want to send a part of other input to avoid overlapping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are two differents input so, when we need to fit the model we have to pass two inputs to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 3s 5ms/step - loss: 1.6079 - val_loss: 2.4401\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.8046 - val_loss: 0.7348\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.6727 - val_loss: 0.6983\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.6242 - val_loss: 0.6405\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.5907 - val_loss: 0.6186\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5634 - val_loss: 0.5941\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5408 - val_loss: 0.5623\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5229 - val_loss: 0.5477\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5069 - val_loss: 0.5374\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4931 - val_loss: 0.5388\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4807 - val_loss: 0.4969\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4716 - val_loss: 0.5055\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4629 - val_loss: 0.4933\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4555 - val_loss: 0.5011\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4496 - val_loss: 0.4883\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4443 - val_loss: 0.4772\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4397 - val_loss: 0.4814\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4358 - val_loss: 0.4677\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4323 - val_loss: 0.4735\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4295 - val_loss: 0.4554\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.4458\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025C30E209D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 124ms/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                        validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.2220306 ],\n",
       "       [0.23647863],\n",
       "       [1.4632845 ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of models are too useful when you wnat to locate some object or person in a picture, and you can adding more outputs layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Subclassing API to Build Dynamic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subclassing API consist in create subclass the Model class, create layers you need in the constructor, and use them to perform the computations you want in the _call()_ method. For example, let's to create a WideAndDeepmodel() as the same the last model we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # handles standard args (e.g., name)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "        \n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"This extra flexibility does come at a cost: your modelâ€™s architecture is hidden within\n",
    "the call() method, so Keras cannot easily inspect it; it cannot save or clone it; and\n",
    "when you call the summary() method, you only get a list of layers, without any information\n",
    "on how they are connected to each other\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Savind ans Restoring a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it used the sequential API is very easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dont run\n",
    "model = keras.models.Sequential([...]) # or keras.Model([...])\n",
    "model.compile([...])\n",
    "model.fit([...])\n",
    "model.save(\"my_keras_model.h5\")# HDF5 save everything abput the model (parameter, layers, weights and biases, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you want to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want waiting until the end of the trainingyou can save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[...] # build and compile the model\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\n",
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you use early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",\n",
    "                                               save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "                    \n",
    "model = keras.models.load_model(\"my_keras_model.h5\") # roll back to best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine both callbacks to save checkpoints of your\n",
    "model (in case your computer crashes) and interrupt training early when there is no\n",
    "more progress (to avoid wasting time and resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorBoard for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool is useful for visualize the learning curves and other parameter of our model. For use it, we need to make a new directory, to save the files regard the training model, cause this output is a binary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to train, let's to use one of the last models we built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\my_logs\\\\run_2022_10_17-10_14_16'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the data\n",
    "fashion_mnits = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnits.load_data()\n",
    "\n",
    "\n",
    "# Split in train, test and validations sets\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Class names\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "# Model using the sequential api\n",
    "model = keras.models.Sequential([\n",
    "                                keras.layers.Flatten(input_shape=[28, 28]),\n",
    "                                keras.layers.Dense(300, activation=\"relu\"),\n",
    "                                keras.layers.Dense(100, activation=\"relu\"),\n",
    "                                keras.layers.Dense(10, activation=\"softmax\")\n",
    "                                ])\n",
    "\n",
    "# Compilation\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "optimizer=\"sgd\",\n",
    "metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until there, all of this is like the last models, but in the fit it's neccessary to add one more step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7152 - accuracy: 0.7650 - val_loss: 0.4970 - val_accuracy: 0.8352\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4910 - accuracy: 0.8283 - val_loss: 0.4646 - val_accuracy: 0.8360\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4465 - accuracy: 0.8433 - val_loss: 0.4932 - val_accuracy: 0.8150\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4192 - accuracy: 0.8537 - val_loss: 0.4073 - val_accuracy: 0.8622\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3990 - accuracy: 0.8607 - val_loss: 0.3762 - val_accuracy: 0.8700\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3834 - accuracy: 0.8650 - val_loss: 0.3849 - val_accuracy: 0.8678\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3682 - accuracy: 0.8702 - val_loss: 0.3703 - val_accuracy: 0.8714\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3563 - accuracy: 0.8744 - val_loss: 0.3587 - val_accuracy: 0.8716\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3465 - accuracy: 0.8772 - val_loss: 0.3571 - val_accuracy: 0.8764\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3364 - accuracy: 0.8800 - val_loss: 0.3829 - val_accuracy: 0.8618\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3286 - accuracy: 0.8826 - val_loss: 0.3418 - val_accuracy: 0.8806\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3199 - accuracy: 0.8853 - val_loss: 0.3324 - val_accuracy: 0.8802\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3136 - accuracy: 0.8876 - val_loss: 0.3287 - val_accuracy: 0.8792\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3063 - accuracy: 0.8899 - val_loss: 0.3438 - val_accuracy: 0.8800\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2997 - accuracy: 0.8929 - val_loss: 0.3195 - val_accuracy: 0.8848\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2937 - accuracy: 0.8947 - val_loss: 0.3366 - val_accuracy: 0.8788\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2874 - accuracy: 0.8970 - val_loss: 0.3127 - val_accuracy: 0.8898\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2823 - accuracy: 0.8983 - val_loss: 0.3117 - val_accuracy: 0.8884\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2777 - accuracy: 0.8991 - val_loss: 0.3044 - val_accuracy: 0.8932\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2719 - accuracy: 0.9008 - val_loss: 0.3014 - val_accuracy: 0.8904\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2670 - accuracy: 0.9045 - val_loss: 0.2986 - val_accuracy: 0.8898\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2619 - accuracy: 0.9055 - val_loss: 0.3146 - val_accuracy: 0.8882\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2567 - accuracy: 0.9078 - val_loss: 0.3028 - val_accuracy: 0.8912\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2521 - accuracy: 0.9093 - val_loss: 0.2940 - val_accuracy: 0.8950\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2480 - accuracy: 0.9107 - val_loss: 0.2966 - val_accuracy: 0.8900\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2437 - accuracy: 0.9121 - val_loss: 0.2966 - val_accuracy: 0.8944\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2394 - accuracy: 0.9139 - val_loss: 0.2972 - val_accuracy: 0.8940\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2347 - accuracy: 0.9162 - val_loss: 0.2909 - val_accuracy: 0.8970\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2319 - accuracy: 0.9163 - val_loss: 0.3007 - val_accuracy: 0.8920\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2281 - accuracy: 0.9172 - val_loss: 0.2882 - val_accuracy: 0.8978\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir) # Assign this new feature\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[tensorboard_cb]) # And aggregate to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like a result we get adirectory structure similar to this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Brayam Pineda\\\\Documents\\\\Learning ANN\\\\Notebooks'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listado de rutas de carpetas para el volumen OS\n",
      "El nï¿½mero de serie del volumen es 3C98-B899\n",
      "C:.\n",
      "+---my_logs\n",
      "    +---run_2022_10_17-10_14_16\n",
      "        +---train\n",
      "        +---validation\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you need to run this command in shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And access to http://localhost:6006/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see in jupyter, you could do ir by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 35472), started 0:26:07 ago. (Use '!kill 35472' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8d369a8b82f4fcf5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8d369a8b82f4fcf5\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wnat to kill the process in windows, you can to run\n",
    "\n",
    "taskkill /IM \"tensorboard.exe\" /F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Neural Network Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first option is training by GridSearchCV or RandomizedSearchCV, but usually the second options is prefered doubt to the amount of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do it, it is necessary to construct a function that will build and compile a Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a KerasRegressor based on this build_model( ) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brayam Pineda\\AppData\\Local\\Temp\\ipykernel_48400\\1709004121.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
     ]
    }
   ],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                validation_data=(X_valid, y_valid),\n",
    "                callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this proccess can during hours depending of the computational capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnd_search_cv.best_estimator_.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are, other alternatives to explore this tuning. Those are based on zooming, a process that explore more a zone if this was good. Libraries which are useful are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hyperopt\n",
    "- Hyperras, kopt or Talos\n",
    "- Keras Tuner\n",
    "- Scikit-Optimize (skopt)\n",
    "- Spearmint\n",
    "- Hyperband\n",
    "- Sklearn-Deap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Learning-ANN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b229deb666612de08db9213da761d2a86d9163fe0b1e327f672b43044cfe1fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
