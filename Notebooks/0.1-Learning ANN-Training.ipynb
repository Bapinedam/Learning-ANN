{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you work with deep neural networks likely you have to be faced to some problems\n",
    "\n",
    "- Vanishing and exploding gradients problems\n",
    "- Not having enough training data\n",
    "- Training may be extremely slow\n",
    "- Overfitting\n",
    "\n",
    "We will go through each of these problem and present techniques to solve them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vanishing/Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, deep neural networks suffer from unstable gradients, diferent layer may learn at widely different speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gorot and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ir require the variance of the input and output be the same.\n",
    "\n",
    "The connection weights of each layer must be initialized randomly.\n",
    "\n",
    "Number of input = _fan-in_\n",
    "NUmber of neurons = _fan-out_\n",
    "\n",
    "Using Gorot initialization can spped up training considerably, and it is one of the tricks that led to the success of deep learning.\n",
    "\n",
    "Some similar strategies has been showed work better with some activatio functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/initializations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default Keras uses Glorot initialization with a uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU activation function used to be mostly used cause it does not saturate for positive values (and because) it is fast to compute\n",
    "\n",
    "Unfortunately, tis function have a big problem called dying ReLUs, during training some neurons 'die', it means they outputting 0 only.\n",
    "\n",
    "One alternative is _leaky ReLU_ and his variants. These variant outperformed ReLU. \n",
    "- Randomized leaky ReLU (RReLU): Alpha is picked randomly, reducing overfitting \n",
    "- Parametric leaky ReLU (PReLU): alpha is learned during training, it is faced like a parameter.\n",
    "\n",
    "Last but not least, the function ELU (_exponential linear unit_) outperformed ReLU too. One variant of this is Scaled ELU (SELU).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"So, which activation function should you use for the hidden layers\n",
    "of your deep neural networks? Although your mileage will vary, in\n",
    "general SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic. If the network’s architecture prevents it from self\n",
    "normalizing,\n",
    "then ELU may perform better than SELU (since SELU\n",
    "is not smooth at z = 0). If you care a lot about runtime latency, then\n",
    "you may prefer leaky ReLU. If you don’t want to tweak yet another\n",
    "hyperparameter, you may use the default a values used by Keras\n",
    "(e.g., 0.3 for leaky ReLU). If you have spare time and computing\n",
    "power, you can use cross-validation to evaluate other activation\n",
    "functions, such as RReLU if your network is overfitting or PReLU\n",
    "if you have a huge training set. That said, because ReLU is the most\n",
    "used activation function (by far), many libraries and hardware\n",
    "accelerators provide ReLU-specific optimizations; therefore, if\n",
    "speed is your priority, ReLU might still be the best choice.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique consist of adding an operation in the model just before or after the activation function each hidden layer. This operation simply zero-centers and normlizes each input.\n",
    "\n",
    "BN also acts like reguarizer reducing the need for other regularization techniques.\n",
    "\n",
    "> You may find that training is rather slow, because each epoch takes\n",
    "much more time when you use Batch Normalization. This is usu\n",
    "ally\n",
    "counterbalanced by the fact that convergence is much faster\n",
    "with BN, so it will take fewer epochs to reach the same perfor\n",
    "mance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Batch normalization in Keras](images\\batchnormalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BatchNormalization class has quite a few hyperparameters you can tweak like momentum. A good momentum is tipically close to 1 (0.9, 0.99, 0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally not a good idea to train a very large DNN from scratch: instead, you should always try to find an existing neural network that accomplishes a similar task to the one you are trying to tackle. This technique is called _transfer learning_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more similar the taks are, the more layers you want to reuse (starting with the lower layers). For very similar tasks, try keeping all the hidden layers and just replacing the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you still cannot get good performance, and you have little training data, try dropping the top hidden layer(s) and freezing all the remaining hidden layers again. You can iterate until you find the right number of layers to reuse. If you have plenty of\n",
    "training data, you may try replacing the top hidden layers instead of dropping them, and even adding more hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must always compile your model after you freeze or unfreeze layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other ways to face the fact you don't have enough data to train you model is to use unsupervised learning and self-supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the mentioned above, one way to optimize the training comes from using faster optimizer than the regular Gradient Descent like: momentum optimization, Nesterov Accelerated Gradient, AdaGrad, RMSProp and finally Adam and Nadam optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In momentum optimizarion, the gradient is used for acceleration, not for speed.\n",
    "\n",
    "Nesterov Accelerated Gradient is a variant of momentum\n",
    "\n",
    "AdaGrad is an adaptative learning faster the traditional gradient and requires much less tuning of the learning rate hyperparameter.\n",
    "\n",
    "RMSProp fixes the problem with the last optimizer, that is tAdaGrad runs the risk of slowing down a bit too fast and never convergind. Except for simple problem, RMSProp is better than AdaGrad.\n",
    "\n",
    "Adam is the preferred optimizer nowdays. Adam stands for _addaptative moment estimation_ combines ideas of momentum and RMSProp. Nadam plus Nesterov trick so it will often converge slightly faster than Adam.\n",
    "\n",
    "Optimizer comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![optimizers comparison](images\\optimizers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the best regularization techniques is early stopping, even batch normalization. The nest are other popular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1 and L2 regularization\n",
    "- Dropout: It is a fairly simple algorithm: at every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability p of being\n",
    "temporarily “dropped out,” meaning it will be entirely ignored during this training step, but it may be active during the next step.\n",
    "- Monte Carlo (MC) Dropout: Based on samples that can be trained too.\n",
    "- Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Practical Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Learning-ANN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b229deb666612de08db9213da761d2a86d9163fe0b1e327f672b43044cfe1fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
