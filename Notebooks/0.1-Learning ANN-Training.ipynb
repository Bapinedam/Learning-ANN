{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you work with deep neural networks likely you have to be faced to some problems\n",
    "\n",
    "- Vanishing and exploding gradients problems\n",
    "- Not having enough training data\n",
    "- Training may be extremely slow\n",
    "- Overfitting\n",
    "\n",
    "We will go through each of these problem and present techniques to solve them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vanishing/Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, deep neural networks suffer from unstable gradients, diferent layer may learn at widely different speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gorot and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It require the variance of the input and output be the same.\n",
    "\n",
    "The connection weights of each layer must be initialized randomly.\n",
    "\n",
    "Number of input = _fan-in_\n",
    "Number of neurons = _fan-out_\n",
    "\n",
    "Using Gorot initialization can speed up training considerably, and it is one of the tricks that led to the success of deep learning.\n",
    "\n",
    "Some similar strategies has been showed work better with particular activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/initializations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default Keras uses Glorot initialization with a uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU activation function used to be mostly used cause it does not saturate for positive values (and because) it is fast to compute\n",
    "\n",
    "Unfortunately, this function have a big problem called dying ReLUs, during training some neurons 'die', it means they outputting 0 only.\n",
    "\n",
    "One alternative is _leaky ReLU_ and his variants. These variant outperformed ReLU. \n",
    "- Randomized leaky ReLU (RReLU): Alpha is picked randomly, reducing overfitting \n",
    "- Parametric leaky ReLU (PReLU): alpha is learned during training, it is faced like a parameter.\n",
    "\n",
    "Last but not least, the function ELU (_exponential linear unit_) outperformed ReLU too. One variant of this is Scaled ELU (SELU).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"So, which activation function should you use for the hidden layers\n",
    "of your deep neural networks? Although your mileage will vary, in\n",
    "general SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic. If the network’s architecture prevents it from self\n",
    "normalizing,\n",
    "then ELU may perform better than SELU (since SELU\n",
    "is not smooth at z = 0). If you care a lot about runtime latency, then\n",
    "you may prefer leaky ReLU. If you don’t want to tweak yet another\n",
    "hyperparameter, you may use the default a values used by Keras\n",
    "(e.g., 0.3 for leaky ReLU). If you have spare time and computing\n",
    "power, you can use cross-validation to evaluate other activation\n",
    "functions, such as RReLU if your network is overfitting or PReLU\n",
    "if you have a huge training set. That said, because ReLU is the most\n",
    "used activation function (by far), many libraries and hardware\n",
    "accelerators provide ReLU-specific optimizations; therefore, if\n",
    "speed is your priority, ReLU might still be the best choice.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique consist of adding an operation in the model just before or after the activation function each hidden layer. This operation simply zero-centers and normlizes each input.\n",
    "\n",
    "BN also acts like reguarizer reducing the need for other regularization techniques.\n",
    "\n",
    "> You may find that training is rather slow, because each epoch takes\n",
    "much more time when you use Batch Normalization. This is usually\n",
    "counterbalanced by the fact that convergence is much faster\n",
    "with BN, so it will take fewer epochs to reach the same perfor\n",
    "mance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BatchNormalization](images\\batchnormalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BatchNormalization class has quite a few hyperparameters you can tweak like momentum. A good momentum is tipically close to 1 (0.9, 0.99, 0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally not a good idea to train a very large DNN from scratch: instead, you should always try to find an existing neural network that accomplishes a similar task to the one you are trying to tackle. This technique is called _transfer learning_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more similar the taks are, the more layers you want to reuse (starting with the lower layers). For very similar tasks, try keeping all the hidden layers and just replacing the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you still cannot get good performance, and you have little training data, try dropping the top hidden layer(s) and freezing all the remaining hidden layers again. You can iterate until you find the right number of layers to reuse. If you have plenty of\n",
    "training data, you may try replacing the top hidden layers instead of dropping them, and even adding more hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must always compile your model after you freeze or unfreeze layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other ways to face the fact you don't have enough data to train you model is to use unsupervised learning and self-supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the mentioned above, one way to optimize the training comes from using faster optimizer than the regular Gradient Descent like: momentum optimization, Nesterov Accelerated Gradient, AdaGrad, RMSProp and finally Adam and Nadam optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In momentum optimization, the gradient is used for acceleration, not for speed.\n",
    "\n",
    "Nesterov Accelerated Gradient is a variant of momentum\n",
    "\n",
    "AdaGrad is an adaptative learning faster the traditional gradient and requires much less tuning of the learning rate hyperparameter.\n",
    "\n",
    "RMSProp fixes the problem with the last optimizer, that is AdaGrad runs the risk of slowing down a bit too fast and never convergind. Except for simple problem, RMSProp is better than AdaGrad.\n",
    "\n",
    "Adam is the preferred optimizer nowdays. Adam stands for _addaptative moment estimation_ combines ideas of momentum and RMSProp. Nadam plus Nesterov trick so it will often converge slightly faster than Adam.\n",
    "\n",
    "Optimizer comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![optimizers](images\\optimizers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the best regularization techniques is early stopping, even batch normalization. The nest are other popular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1 and L2 regularization\n",
    "- Dropout: It is a fairly simple algorithm: at every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability p of being\n",
    "temporarily “dropped out,” meaning it will be entirely ignored during this training step, but it may be active during the next step.\n",
    "- Monte Carlo (MC) Dropout: Based on samples that can be trained too.\n",
    "- Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Practical Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, there is not a consensus about the best configuration of this techniques, there are one that work fine in most of the cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Configuraciones](images\\configuration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the network is a simple stack of dense layers, then it can self-normalize, and you should the next configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Self-normalization](images\\selfnormalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural network on the CIFAR10 image dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the practice it's necessary to do a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = x_train[:5000], x_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 32, 32, 3), (45000, 32, 32, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but\n",
    "it’s the point of this exercise). Use He initialization and the ELU activation\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[32, 32, 3]),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using Nadam optimization and early stopping, train the network on the\n",
    "CIFAR10 dataset. Remember to search for the right learning rate each\n",
    "time you change the model’s architecture or hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nadam optimization\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilamos el modelo\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlystopping\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
    "# Checkpoint\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"cifar10_keras_model.h5\",\n",
    "                                                save_best_only=True)\n",
    "# Tensorboard\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard --logdir=./my_cifar10_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 16s 9ms/step - loss: 1.7009 - accuracy: 0.3785 - val_loss: 1.7205 - val_accuracy: 0.3682\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.6896 - accuracy: 0.3815 - val_loss: 1.7122 - val_accuracy: 0.3736\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.6834 - accuracy: 0.3821 - val_loss: 1.7151 - val_accuracy: 0.3736\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.6788 - accuracy: 0.3842 - val_loss: 1.7084 - val_accuracy: 0.3736\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.6754 - accuracy: 0.3871 - val_loss: 1.7074 - val_accuracy: 0.3784\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.6713 - accuracy: 0.3863 - val_loss: 1.7045 - val_accuracy: 0.3766\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.6667 - accuracy: 0.3886 - val_loss: 1.7017 - val_accuracy: 0.3822\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.6625 - accuracy: 0.3910 - val_loss: 1.6974 - val_accuracy: 0.3822\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.6597 - accuracy: 0.3922 - val_loss: 1.6962 - val_accuracy: 0.3838\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.6554 - accuracy: 0.3955 - val_loss: 1.6980 - val_accuracy: 0.3776\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.6527 - accuracy: 0.3957 - val_loss: 1.6952 - val_accuracy: 0.3852\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.9442 - accuracy: 0.3950 - val_loss: 1.6928 - val_accuracy: 0.3860\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.6469 - accuracy: 0.3965 - val_loss: 1.6897 - val_accuracy: 0.3870\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.6441 - accuracy: 0.3990 - val_loss: 1.6895 - val_accuracy: 0.3874\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.6417 - accuracy: 0.3989 - val_loss: 1.6907 - val_accuracy: 0.3838\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.6402 - accuracy: 0.3990 - val_loss: 1.6908 - val_accuracy: 0.3846\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.6385 - accuracy: 0.4010 - val_loss: 1.6884 - val_accuracy: 0.3882\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.6365 - accuracy: 0.4023 - val_loss: 1.6860 - val_accuracy: 0.3838\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.6343 - accuracy: 0.4023 - val_loss: 1.6837 - val_accuracy: 0.3902\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.6339 - accuracy: 0.4024 - val_loss: 1.6834 - val_accuracy: 0.3870\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.6738 - accuracy: 0.3958 - val_loss: 1.6876 - val_accuracy: 0.3888\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.6353 - accuracy: 0.4006 - val_loss: 1.6817 - val_accuracy: 0.3846\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.6295 - accuracy: 0.4037 - val_loss: 1.6841 - val_accuracy: 0.3872\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.6262 - accuracy: 0.4052 - val_loss: 1.6810 - val_accuracy: 0.3846\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.6240 - accuracy: 0.4057 - val_loss: 1.6844 - val_accuracy: 0.3868\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.6215 - accuracy: 0.4060 - val_loss: 1.6807 - val_accuracy: 0.3892\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.6199 - accuracy: 0.4081 - val_loss: 1.6801 - val_accuracy: 0.3904\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.6182 - accuracy: 0.4077 - val_loss: 1.6813 - val_accuracy: 0.3886\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.6159 - accuracy: 0.4086 - val_loss: 1.6789 - val_accuracy: 0.3912\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.6136 - accuracy: 0.4099 - val_loss: 1.6847 - val_accuracy: 0.3820\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.6117 - accuracy: 0.4106 - val_loss: 1.6791 - val_accuracy: 0.3908\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.6097 - accuracy: 0.4104 - val_loss: 1.6827 - val_accuracy: 0.3874\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.6063 - accuracy: 0.4128 - val_loss: 1.6778 - val_accuracy: 0.3938\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.6043 - accuracy: 0.4130 - val_loss: 1.6754 - val_accuracy: 0.3908\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.6021 - accuracy: 0.4123 - val_loss: 1.6744 - val_accuracy: 0.3904\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5991 - accuracy: 0.4155 - val_loss: 1.6780 - val_accuracy: 0.3914\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5974 - accuracy: 0.4164 - val_loss: 1.6751 - val_accuracy: 0.3912\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.5943 - accuracy: 0.4167 - val_loss: 1.6716 - val_accuracy: 0.3898\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.5915 - accuracy: 0.4186 - val_loss: 1.6837 - val_accuracy: 0.3902\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5899 - accuracy: 0.4185 - val_loss: 1.6746 - val_accuracy: 0.3952\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5879 - accuracy: 0.4184 - val_loss: 1.6751 - val_accuracy: 0.3892\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5850 - accuracy: 0.4199 - val_loss: 1.6756 - val_accuracy: 0.3918\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.6236 - accuracy: 0.4082 - val_loss: 1.6766 - val_accuracy: 0.3922\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.5918 - accuracy: 0.4173 - val_loss: 1.6756 - val_accuracy: 0.3928\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.5857 - accuracy: 0.4192 - val_loss: 1.6706 - val_accuracy: 0.3926\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5813 - accuracy: 0.4223 - val_loss: 1.6750 - val_accuracy: 0.3908\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5778 - accuracy: 0.4239 - val_loss: 1.6719 - val_accuracy: 0.3968\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5742 - accuracy: 0.4245 - val_loss: 1.6708 - val_accuracy: 0.3960\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5718 - accuracy: 0.4254 - val_loss: 1.6723 - val_accuracy: 0.3934\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.5709 - accuracy: 0.4257 - val_loss: 1.6733 - val_accuracy: 0.3922\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.5677 - accuracy: 0.4266 - val_loss: 1.6716 - val_accuracy: 0.3930\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.5655 - accuracy: 0.4274 - val_loss: 1.6744 - val_accuracy: 0.4012\n",
      "Epoch 53/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5636 - accuracy: 0.4279 - val_loss: 1.6720 - val_accuracy: 0.4012\n",
      "Epoch 54/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5606 - accuracy: 0.4290 - val_loss: 1.6728 - val_accuracy: 0.3952\n",
      "Epoch 55/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5581 - accuracy: 0.4314 - val_loss: 1.6733 - val_accuracy: 0.3970\n",
      "Epoch 56/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.5565 - accuracy: 0.4311 - val_loss: 1.6731 - val_accuracy: 0.3974\n",
      "Epoch 57/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5534 - accuracy: 0.4336 - val_loss: 1.6731 - val_accuracy: 0.3972\n",
      "Epoch 58/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5509 - accuracy: 0.4346 - val_loss: 1.6776 - val_accuracy: 0.4002\n",
      "Epoch 59/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5486 - accuracy: 0.4340 - val_loss: 1.6814 - val_accuracy: 0.3940\n",
      "Epoch 60/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.5467 - accuracy: 0.4351 - val_loss: 1.6801 - val_accuracy: 0.3972\n",
      "Epoch 61/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.5450 - accuracy: 0.4363 - val_loss: 1.6769 - val_accuracy: 0.3964\n",
      "Epoch 62/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5411 - accuracy: 0.4378 - val_loss: 1.6788 - val_accuracy: 0.3946\n",
      "Epoch 63/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5384 - accuracy: 0.4397 - val_loss: 1.6828 - val_accuracy: 0.3946\n",
      "Epoch 64/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.5381 - accuracy: 0.4387 - val_loss: 1.6836 - val_accuracy: 0.3964\n",
      "Epoch 65/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5345 - accuracy: 0.4396 - val_loss: 1.6852 - val_accuracy: 0.3932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2000cf9fc70>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100,\n",
    "            validation_data=(X_valid, y_valid),\n",
    "            callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now try adding Batch Normalization and compare the learning curves: Is it\n",
    "converging faster than before? Does it produce a better model? How does it\n",
    "affect training speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 3072)              0         \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 3072)             12288     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 100)               307300    \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_30 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_31 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_32 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_35 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_36 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_37 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_38 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_39 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 520,098\n",
      "Trainable params: 510,154\n",
      "Non-trainable params: 9,944\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bn = keras.models.Sequential()\n",
    "\n",
    "model_bn.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "for i in range(20):\n",
    "    model_bn.add(keras.layers.BatchNormalization())\n",
    "    model_bn.add(keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'))\n",
    "\n",
    "model_bn.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 65s 26ms/step - loss: 2.0633 - accuracy: 0.2634 - val_loss: 1.8141 - val_accuracy: 0.3420\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.8514 - accuracy: 0.3372 - val_loss: 1.7263 - val_accuracy: 0.3700\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.7753 - accuracy: 0.3645 - val_loss: 1.6641 - val_accuracy: 0.3976\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.7151 - accuracy: 0.3880 - val_loss: 1.6187 - val_accuracy: 0.4056\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.6740 - accuracy: 0.3994 - val_loss: 1.5915 - val_accuracy: 0.4268\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.6299 - accuracy: 0.4180 - val_loss: 1.5533 - val_accuracy: 0.4486\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.5975 - accuracy: 0.4282 - val_loss: 1.5253 - val_accuracy: 0.4570\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.5711 - accuracy: 0.4423 - val_loss: 1.4980 - val_accuracy: 0.4664\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.5431 - accuracy: 0.4522 - val_loss: 1.4920 - val_accuracy: 0.4548\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.5176 - accuracy: 0.4602 - val_loss: 1.4802 - val_accuracy: 0.4664\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.5030 - accuracy: 0.4636 - val_loss: 1.4635 - val_accuracy: 0.4772\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.4831 - accuracy: 0.4726 - val_loss: 1.4479 - val_accuracy: 0.4832\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.4588 - accuracy: 0.4829 - val_loss: 1.4572 - val_accuracy: 0.4794\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.4424 - accuracy: 0.4882 - val_loss: 1.4421 - val_accuracy: 0.4906\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.4250 - accuracy: 0.4936 - val_loss: 1.4240 - val_accuracy: 0.5012\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.4120 - accuracy: 0.4985 - val_loss: 1.4118 - val_accuracy: 0.4996\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 37s 26ms/step - loss: 1.3937 - accuracy: 0.5040 - val_loss: 1.4103 - val_accuracy: 0.5022\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.3821 - accuracy: 0.5090 - val_loss: 1.4103 - val_accuracy: 0.5024\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 37s 26ms/step - loss: 1.3731 - accuracy: 0.5128 - val_loss: 1.4001 - val_accuracy: 0.5072\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 37s 26ms/step - loss: 1.3535 - accuracy: 0.5174 - val_loss: 1.4015 - val_accuracy: 0.5082\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 39s 27ms/step - loss: 1.3440 - accuracy: 0.5218 - val_loss: 1.4059 - val_accuracy: 0.5052\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.3283 - accuracy: 0.5283 - val_loss: 1.3913 - val_accuracy: 0.5038\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 39s 27ms/step - loss: 1.3248 - accuracy: 0.5293 - val_loss: 1.4015 - val_accuracy: 0.5056\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.3103 - accuracy: 0.5323 - val_loss: 1.3958 - val_accuracy: 0.5112\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 39s 27ms/step - loss: 1.3037 - accuracy: 0.5377 - val_loss: 1.3880 - val_accuracy: 0.5148\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.2909 - accuracy: 0.5425 - val_loss: 1.3945 - val_accuracy: 0.5154\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.2889 - accuracy: 0.5428 - val_loss: 1.3831 - val_accuracy: 0.5168\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.2782 - accuracy: 0.5465 - val_loss: 1.3811 - val_accuracy: 0.5138\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.2664 - accuracy: 0.5502 - val_loss: 1.3750 - val_accuracy: 0.5176\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.2526 - accuracy: 0.5560 - val_loss: 1.3893 - val_accuracy: 0.5102\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.2424 - accuracy: 0.5604 - val_loss: 1.3740 - val_accuracy: 0.5202\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 35s 25ms/step - loss: 1.2400 - accuracy: 0.5614 - val_loss: 1.3845 - val_accuracy: 0.5126\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 33s 24ms/step - loss: 1.2311 - accuracy: 0.5638 - val_loss: 1.3819 - val_accuracy: 0.5130\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.2285 - accuracy: 0.5644 - val_loss: 1.3727 - val_accuracy: 0.5274\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 37s 26ms/step - loss: 1.2116 - accuracy: 0.5699 - val_loss: 1.3807 - val_accuracy: 0.5184\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.2067 - accuracy: 0.5711 - val_loss: 1.3910 - val_accuracy: 0.5190\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.1979 - accuracy: 0.5746 - val_loss: 1.3827 - val_accuracy: 0.5180\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.1978 - accuracy: 0.5765 - val_loss: 1.3681 - val_accuracy: 0.5224\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 36s 26ms/step - loss: 1.1807 - accuracy: 0.5823 - val_loss: 1.3737 - val_accuracy: 0.5192\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 37s 26ms/step - loss: 1.1830 - accuracy: 0.5796 - val_loss: 1.3865 - val_accuracy: 0.5196\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 37s 26ms/step - loss: 1.1721 - accuracy: 0.5855 - val_loss: 1.3924 - val_accuracy: 0.5156\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 37s 26ms/step - loss: 1.1643 - accuracy: 0.5866 - val_loss: 1.3833 - val_accuracy: 0.5184\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.1563 - accuracy: 0.5891 - val_loss: 1.3912 - val_accuracy: 0.5232\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 36s 26ms/step - loss: 1.1573 - accuracy: 0.5893 - val_loss: 1.3889 - val_accuracy: 0.5192\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 35s 25ms/step - loss: 1.1440 - accuracy: 0.5943 - val_loss: 1.3825 - val_accuracy: 0.5292\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.1388 - accuracy: 0.5950 - val_loss: 1.3910 - val_accuracy: 0.5168\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 39s 27ms/step - loss: 1.1293 - accuracy: 0.5985 - val_loss: 1.3905 - val_accuracy: 0.5188\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.1273 - accuracy: 0.6019 - val_loss: 1.3841 - val_accuracy: 0.5216\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 36s 26ms/step - loss: 1.1221 - accuracy: 0.6018 - val_loss: 1.4054 - val_accuracy: 0.5252\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.1175 - accuracy: 0.6038 - val_loss: 1.4091 - val_accuracy: 0.5188\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.1112 - accuracy: 0.6063 - val_loss: 1.4010 - val_accuracy: 0.5234\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 35s 25ms/step - loss: 1.0995 - accuracy: 0.6060 - val_loss: 1.4009 - val_accuracy: 0.5230\n",
      "Epoch 53/100\n",
      "1407/1407 [==============================] - 35s 25ms/step - loss: 1.0899 - accuracy: 0.6119 - val_loss: 1.4033 - val_accuracy: 0.5188\n",
      "Epoch 54/100\n",
      "1407/1407 [==============================] - 36s 26ms/step - loss: 1.0915 - accuracy: 0.6126 - val_loss: 1.4041 - val_accuracy: 0.5170\n",
      "Epoch 55/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.0863 - accuracy: 0.6139 - val_loss: 1.4012 - val_accuracy: 0.5172\n",
      "Epoch 56/100\n",
      "1407/1407 [==============================] - 34s 24ms/step - loss: 1.0821 - accuracy: 0.6175 - val_loss: 1.3992 - val_accuracy: 0.5204\n",
      "Epoch 57/100\n",
      "1407/1407 [==============================] - 36s 26ms/step - loss: 1.0709 - accuracy: 0.6223 - val_loss: 1.4097 - val_accuracy: 0.5180\n",
      "Epoch 58/100\n",
      "1407/1407 [==============================] - 36s 25ms/step - loss: 1.0721 - accuracy: 0.6183 - val_loss: 1.4083 - val_accuracy: 0.5240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20013ace280>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compilamos el modelo\n",
    "\n",
    "model_bn.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"cifar10_keras_model_bn.h5\",\n",
    "                                                save_best_only=True)\n",
    "# Tensorboard\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_nb_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model_bn.fit(X_train, y_train, epochs=100,\n",
    "            validation_data=(X_valid, y_valid),\n",
    "            callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sense of the speed, it is true that the number of epoch is less, but in consideration with the speed of the train whit out batch normalization, the first was faster. In order to quality of the model, not doubt the batch normalization model outperfomed the first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Try replacing Batch Normalization with SELU, and make the necessary adjustements\n",
    "to ensure the network self-normalizes (i.e., standardize the input features,\n",
    "use LeCun normal initialization, make sure the DNN contains only a\n",
    "sequence of dense layers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 18s 10ms/step - loss: 1.9404 - accuracy: 0.2901 - val_loss: 1.8995 - val_accuracy: 0.3166\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.7578 - accuracy: 0.3634 - val_loss: 1.7260 - val_accuracy: 0.3752\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.6771 - accuracy: 0.3911 - val_loss: 1.6774 - val_accuracy: 0.3854\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.6181 - accuracy: 0.4168 - val_loss: 1.8088 - val_accuracy: 0.3592\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.5761 - accuracy: 0.4319 - val_loss: 1.5725 - val_accuracy: 0.4328\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5429 - accuracy: 0.4476 - val_loss: 1.5366 - val_accuracy: 0.4452\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5120 - accuracy: 0.4561 - val_loss: 1.5291 - val_accuracy: 0.4486\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.4880 - accuracy: 0.4659 - val_loss: 1.6090 - val_accuracy: 0.4152\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.4640 - accuracy: 0.4743 - val_loss: 1.5510 - val_accuracy: 0.4410\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.4399 - accuracy: 0.4824 - val_loss: 1.4973 - val_accuracy: 0.4608\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.4226 - accuracy: 0.4884 - val_loss: 1.5679 - val_accuracy: 0.4488\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.4044 - accuracy: 0.4965 - val_loss: 1.4956 - val_accuracy: 0.4596\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3901 - accuracy: 0.4991 - val_loss: 1.5402 - val_accuracy: 0.4472\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.3708 - accuracy: 0.5079 - val_loss: 1.5144 - val_accuracy: 0.4638\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.3584 - accuracy: 0.5104 - val_loss: 1.4722 - val_accuracy: 0.4696\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.3439 - accuracy: 0.5162 - val_loss: 1.5454 - val_accuracy: 0.4580\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3293 - accuracy: 0.5221 - val_loss: 1.5040 - val_accuracy: 0.4636\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.3159 - accuracy: 0.5237 - val_loss: 1.4892 - val_accuracy: 0.4688\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3008 - accuracy: 0.5308 - val_loss: 1.4637 - val_accuracy: 0.4850\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2905 - accuracy: 0.5362 - val_loss: 1.4641 - val_accuracy: 0.4838\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2790 - accuracy: 0.5400 - val_loss: 1.4783 - val_accuracy: 0.4744\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2654 - accuracy: 0.5432 - val_loss: 1.4850 - val_accuracy: 0.4746\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2550 - accuracy: 0.5491 - val_loss: 1.4739 - val_accuracy: 0.4762\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2423 - accuracy: 0.5515 - val_loss: 1.4589 - val_accuracy: 0.4890\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2301 - accuracy: 0.5568 - val_loss: 1.4574 - val_accuracy: 0.4864\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2197 - accuracy: 0.5625 - val_loss: 1.4706 - val_accuracy: 0.4834\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2099 - accuracy: 0.5648 - val_loss: 1.4582 - val_accuracy: 0.4926\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1968 - accuracy: 0.5701 - val_loss: 1.4647 - val_accuracy: 0.4808\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1864 - accuracy: 0.5738 - val_loss: 1.5338 - val_accuracy: 0.4712\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1751 - accuracy: 0.5778 - val_loss: 1.4995 - val_accuracy: 0.4794\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1643 - accuracy: 0.5766 - val_loss: 1.4614 - val_accuracy: 0.4890\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1540 - accuracy: 0.5840 - val_loss: 1.5139 - val_accuracy: 0.4870\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1435 - accuracy: 0.5874 - val_loss: 1.5057 - val_accuracy: 0.4788\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1337 - accuracy: 0.5907 - val_loss: 1.4836 - val_accuracy: 0.4888\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1255 - accuracy: 0.5938 - val_loss: 1.5023 - val_accuracy: 0.4816\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1149 - accuracy: 0.5980 - val_loss: 1.4787 - val_accuracy: 0.4910\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1070 - accuracy: 0.6017 - val_loss: 1.4982 - val_accuracy: 0.4894\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0969 - accuracy: 0.6041 - val_loss: 1.5104 - val_accuracy: 0.4896\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0867 - accuracy: 0.6076 - val_loss: 1.4927 - val_accuracy: 0.4938\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0730 - accuracy: 0.6120 - val_loss: 1.5152 - val_accuracy: 0.4896\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0663 - accuracy: 0.6172 - val_loss: 1.5058 - val_accuracy: 0.4984\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0551 - accuracy: 0.6195 - val_loss: 1.5371 - val_accuracy: 0.4848\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0419 - accuracy: 0.6254 - val_loss: 1.5304 - val_accuracy: 0.4872\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0380 - accuracy: 0.6266 - val_loss: 1.5335 - val_accuracy: 0.4914\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0306 - accuracy: 0.6284 - val_loss: 1.5126 - val_accuracy: 0.4916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25d912809a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selu = keras.models.Sequential()\n",
    "\n",
    "# Nadam optimization\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.00005)\n",
    "\n",
    "model_selu.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "for i in range(20):\n",
    "    model_selu.add(keras.layers.Dense(100, activation='selu', kernel_initializer='lecun_normal'))\n",
    "\n",
    "model_selu.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compilamos el modelo\n",
    "\n",
    "model_selu.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Earlystopping\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"cifar10_keras_model_SELU.h5\",\n",
    "                                                save_best_only=True)\n",
    "# Tensorboard\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_SELU_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model_selu.fit(X_train, y_train, epochs=100,\n",
    "            validation_data=(X_valid, y_valid),\n",
    "            callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achives a smallers improves in the metric. The change in the metric is too long but if you apply a grid it may improve more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Try regularizing the model with alpha dropout. Then, without retraining your\n",
    "model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 20s 9ms/step - loss: 2.4997 - accuracy: 0.1862 - val_loss: 2.3492 - val_accuracy: 0.3572\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 2.0860 - accuracy: 0.2764 - val_loss: 2.1809 - val_accuracy: 0.3864\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.8830 - accuracy: 0.3369 - val_loss: 2.0934 - val_accuracy: 0.4074\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.7443 - accuracy: 0.3822 - val_loss: 2.1377 - val_accuracy: 0.4264\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.6592 - accuracy: 0.4106 - val_loss: 2.0787 - val_accuracy: 0.4376\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.5966 - accuracy: 0.4340 - val_loss: 1.9907 - val_accuracy: 0.4432\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.5356 - accuracy: 0.4574 - val_loss: 2.0240 - val_accuracy: 0.4500\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.4908 - accuracy: 0.4756 - val_loss: 2.0291 - val_accuracy: 0.4562\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.4515 - accuracy: 0.4874 - val_loss: 2.0477 - val_accuracy: 0.4566\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.4134 - accuracy: 0.4992 - val_loss: 2.0520 - val_accuracy: 0.4610\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3803 - accuracy: 0.5132 - val_loss: 2.1019 - val_accuracy: 0.4782\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3411 - accuracy: 0.5259 - val_loss: 2.1764 - val_accuracy: 0.4662\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3205 - accuracy: 0.5315 - val_loss: 2.2866 - val_accuracy: 0.4696\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2941 - accuracy: 0.5445 - val_loss: 2.1545 - val_accuracy: 0.4726\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2646 - accuracy: 0.5524 - val_loss: 2.2992 - val_accuracy: 0.4788\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2436 - accuracy: 0.5582 - val_loss: 2.3573 - val_accuracy: 0.4708\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2169 - accuracy: 0.5693 - val_loss: 2.5006 - val_accuracy: 0.4774\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1949 - accuracy: 0.5782 - val_loss: 2.4506 - val_accuracy: 0.4696\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1744 - accuracy: 0.5853 - val_loss: 2.6870 - val_accuracy: 0.4794\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1490 - accuracy: 0.5954 - val_loss: 2.6981 - val_accuracy: 0.4748\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1279 - accuracy: 0.6004 - val_loss: 2.8588 - val_accuracy: 0.4762\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1131 - accuracy: 0.6044 - val_loss: 2.9479 - val_accuracy: 0.4722\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0934 - accuracy: 0.6122 - val_loss: 2.9565 - val_accuracy: 0.4792\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0750 - accuracy: 0.6188 - val_loss: 2.9670 - val_accuracy: 0.4724\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0580 - accuracy: 0.6235 - val_loss: 3.1423 - val_accuracy: 0.4764\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0365 - accuracy: 0.6324 - val_loss: 3.0743 - val_accuracy: 0.4728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c04605ce50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_alpha = keras.models.Sequential()\n",
    "\n",
    "# Nadam optimization\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.00005)\n",
    "\n",
    "model_alpha.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "for i in range(17):\n",
    "    model_alpha.add(keras.layers.Dense(100, activation='selu', kernel_initializer='lecun_normal'))\n",
    "\n",
    "for i in range(3):\n",
    "    model_alpha.add(keras.layers.Dense(100, activation='selu', kernel_initializer='lecun_normal'))\n",
    "    model_alpha.add(keras.layers.AlphaDropout(0.30))\n",
    "\n",
    "model_alpha.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compilamos el modelo\n",
    "\n",
    "model_alpha.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Earlystopping\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"cifar10_keras_model_alpha.h5\",\n",
    "                                                save_best_only=True)\n",
    "# Tensorboard\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_ALPHA_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (x_test - X_means) / X_stds\n",
    "\n",
    "model_alpha.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 6ms/step - loss: 1.9907 - accuracy: 0.4432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9906935691833496, 0.4431999921798706]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"cifar10_keras_model_alpha.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't see slightly improving in this model. It'd be curious to develop a model whose achieve a higher performance in this task, or search for one model whose achieve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 2s 10ms/step\n",
      "157/157 [==============================] - 2s 11ms/step\n",
      "157/157 [==============================] - 1s 8ms/step\n",
      "157/157 [==============================] - 2s 9ms/step\n",
      "157/157 [==============================] - 1s 8ms/step\n",
      "157/157 [==============================] - 2s 11ms/step\n",
      "157/157 [==============================] - 1s 5ms/step\n",
      "157/157 [==============================] - 1s 5ms/step\n",
      "157/157 [==============================] - 1s 7ms/step\n",
      "157/157 [==============================] - 2s 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4484"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It got not a better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Learning-ANN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b229deb666612de08db9213da761d2a86d9163fe0b1e327f672b43044cfe1fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
